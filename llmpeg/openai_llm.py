"""Implementation of LLMInterface for OpenAI LLM API."""
from openai import OpenAI
from typing import List, Dict
from llmpeg.llm_interface import LLMInterface


class OpenAILLMInterface(LLMInterface):
    """Implementation of LLMInterface for OpenAI LLM API."""

    def __init__(self, model_string: str):
        """Initialize OpenAI API connection and create message history.

        Args:
            model_string (str): The specific model to be used in API.
        """
        self._model_string = model_string

        self.client = OpenAI()

        self.history: List[Dict[str, str]] = []

    def add_system_prompt(self, prompt: str):
        """Add a system prompt to the model's context.

        Uses the format outlined in OpenAI's LLM API.

        Args:
            prompt (str): The system prompt text to be added.
        """
        self.history.append({"role": "system", "content": prompt})

    def add_assistant_prompt(self, prompt: str):
        """Add an assistant prompt to the model's context.

        Uses the format outlined in OpenAI's LLM API.

        Args:
            prompt (str): The assistant prompt text to be added.
        """
        self.history.append({"role": "assistant", "content": prompt})

    def add_user_prompt(self, prompt: str):
        """Add a user prompt to the model's context.

        Uses the format outlined in OpenAI's LLM API.

        Args:
            prompt (str): The user prompt text to be added.
        """
        self.history.append({"role": "user", "content": prompt})

    def invoke_model(self) -> str:
        """Invoke the OpenAI LLM Chat API to obtain response.

        Species the return type to be a json object. Also
        specifies the specific API model type to use.

        Returns:
            str: The output generated by the language model.
        """
        response = self.client.chat.completions.create(
            model=self._model_string,
            response_format={"type": "json_object"},
            messages=self.history,
            temperature=0.2,
            top_p=0.2,
        )

        # Extract the LLM's decision from the response
        raw_json_string = response.choices[0].message.content

        return raw_json_string
